{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "wanted-grade",
   "metadata": {
    "id": "wanted-grade"
   },
   "source": [
    "# Run inference\n",
    "Ref: http://trillium.tech/ml4floods/content/ml4ops/HOWTO_Run_Inference_on_new_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "encouraging-whole",
   "metadata": {
    "id": "encouraging-whole",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "from models import flooding_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "instrumental-brake",
   "metadata": {
    "id": "instrumental-brake"
   },
   "source": [
    "## Step 1: Get config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tutorial-coordinator",
   "metadata": {
    "id": "tutorial-coordinator",
    "outputId": "4495dddb-521b-46d2-e9ea-e90fa6975c0b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Config for experiment:  only_water\n",
      "{   'data_params': {   'batch_size': 80,\n",
      "                       'bucket_id': '',\n",
      "                       'channel_configuration': 'bgri',\n",
      "                       'download': {'test': True, 'train': True, 'val': True},\n",
      "                       'filter_windows': {   'apply': False,\n",
      "                                             'threshold_clouds': 0.5,\n",
      "                                             'version': 'v1'},\n",
      "                       'input_folder': 'S2',\n",
      "                       'loader_type': 'local',\n",
      "                       'num_workers': 16,\n",
      "                       'path_to_splits': '/media/hsu/Disk/dataset/Flooding//worldfloods_v1_0',\n",
      "                       'target_folder': 'gt',\n",
      "                       'test_transformation': {'normalize': True},\n",
      "                       'train_test_split_file': '/home/hsu/Projects/train_test_split.json',\n",
      "                       'train_transformation': {'normalize': True},\n",
      "                       'window_size': [256, 256]},\n",
      "    'experiment_name': 'only_water',\n",
      "    'gpus': '0',\n",
      "    'model_params': {   'hyperparameters': {   'channel_configuration': 'bgri',\n",
      "                                               'early_stopping_patience': 4,\n",
      "                                               'label_names': [   'land',\n",
      "                                                                  'water',\n",
      "                                                                  'cloud'],\n",
      "                                               'lr': 0.0001,\n",
      "                                               'lr_decay': 0.5,\n",
      "                                               'lr_patience': 2,\n",
      "                                               'max_epochs': 20,\n",
      "                                               'max_tile_size': 256,\n",
      "                                               'metric_monitor': 'val_dice_loss',\n",
      "                                               'model_type': 'res2vtunet',\n",
      "                                               'num_channels': 4,\n",
      "                                               'num_classes': 3,\n",
      "                                               'val_every': 1,\n",
      "                                               'weight_per_class': [   1.93445299,\n",
      "                                                                       36.60054169,\n",
      "                                                                       2.19400729]},\n",
      "                        'max_tile_size': 256,\n",
      "                        'model_folder': 'train_models',\n",
      "                        'model_version': 'v1',\n",
      "                        'test': False,\n",
      "                        'train': True},\n",
      "    'resume_from_checkpoint': False,\n",
      "    'seed': 12}\n"
     ]
    }
   ],
   "source": [
    "from ml4floods.models.config_setup import get_default_config\n",
    "\n",
    "experiment_name = \"WFV1_unet\"\n",
    "prod_dev = \"2_PROD\"\n",
    "\n",
    "# config_fp = \"train_models/training_flooding/config_rgbnir.json\"\n",
    "config_fp = \"./train_models/only_water/config_rgbnirh_worldflood2_res2vtunet_biou+dice.json\"\n",
    "\n",
    "config = get_default_config(config_fp)\n",
    "config.data_params.data_params='ml4cc_data_lake'\n",
    "\n",
    "# The max_tile_size param controls the max size of patches that are fed to the NN. If you're in a memory contrained environment set this value to 128\n",
    "# config[\"model_params\"][\"max_tile_size\"] = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "municipal-variable",
   "metadata": {
    "id": "municipal-variable"
   },
   "source": [
    "## Step 2: Load pre-trained model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59d3942b-c716-4d91-9bf9-a05e0280fcf5",
   "metadata": {},
   "source": [
    "from ml4floods.models.model_setup import get_model\n",
    "\n",
    "config[\"model_params\"]['model_folder'] = f'gs://ml4cc_data_lake/{prod_dev}/2_Mart/2_MLModelMart'\n",
    "config[\"model_params\"]['test'] = True\n",
    "model = get_model(config.model_params, experiment_name)\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cuda\") # comment this line if your machine does not have GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d39cff0-4ce9-475e-b688-2155c0da8dc8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_models/only_water/model_rgbnirh_worldflood2_res2vtunet_biou+dice.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WorldFloodsModel(\n",
       "  (network): Res2VTUnet(\n",
       "    (in_conv): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (cvt1): CvTblock(\n",
       "      (conv_embed): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(4, 4), padding=(1, 1))\n",
       "        (1): Rearrange('b c h w -> b (h w) c', h=64, w=64)\n",
       "        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (transformer): Sequential(\n",
       "        (0): Trans_depth(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x TransformerBlock(\n",
       "              (mha): Res2Attention(\n",
       "                (res2): Res2Net(\n",
       "                  (inconv): Sequential(\n",
       "                    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (x3conv): Sequential(\n",
       "                    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "                    (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "                (hue): RGB2HSV()\n",
       "                (to_q): SepConv2d(\n",
       "                  (depthwise): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)\n",
       "                  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (pointwise): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (to_k): SepConv2d(\n",
       "                  (depthwise): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)\n",
       "                  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (pointwise): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (to_v): SepConv2d(\n",
       "                  (depthwise): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)\n",
       "                  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (pointwise): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (linear): Linear(in_features=1, out_features=8, bias=False)\n",
       "                (ln_v): Linear(in_features=8, out_features=256, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (avg_pool): AdaptiveAvgPool2d(output_size=16)\n",
       "                (max_pool): AdaptiveMaxPool2d(output_size=16)\n",
       "                (out_attention): Linear(in_features=64, out_features=64, bias=False)\n",
       "                (drop): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (mlp_layers): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                  (2): Dropout(p=0.1, inplace=False)\n",
       "                  (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "                  (4): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Rearrange('b (h w) c -> b c h w', h=64, w=64)\n",
       "      )\n",
       "    )\n",
       "    (cvt2): CvTblock(\n",
       "      (conv_embed): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): Rearrange('b c h w -> b (h w) c', h=32, w=32)\n",
       "        (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (transformer): Sequential(\n",
       "        (0): Trans_depth(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x TransformerBlock(\n",
       "              (mha): Res2Attention(\n",
       "                (res2): Res2Net(\n",
       "                  (inconv): Sequential(\n",
       "                    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (x3conv): Sequential(\n",
       "                    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "                    (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "                (hue): RGB2HSV()\n",
       "                (to_q): SepConv2d(\n",
       "                  (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "                  (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (pointwise): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (to_k): SepConv2d(\n",
       "                  (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "                  (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (pointwise): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (to_v): SepConv2d(\n",
       "                  (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "                  (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (pointwise): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (linear): Linear(in_features=1, out_features=16, bias=False)\n",
       "                (ln_v): Linear(in_features=16, out_features=64, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (1): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (avg_pool): AdaptiveAvgPool2d(output_size=8)\n",
       "                (max_pool): AdaptiveMaxPool2d(output_size=8)\n",
       "                (out_attention): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (drop): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (mlp_layers): Sequential(\n",
       "                  (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                  (2): Dropout(p=0.1, inplace=False)\n",
       "                  (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "                  (4): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Rearrange('b (h w) c -> b c h w', h=32, w=32)\n",
       "      )\n",
       "    )\n",
       "    (cvt3): CvTblock(\n",
       "      (conv_embed): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): Rearrange('b c h w -> b (h w) c', h=16, w=16)\n",
       "        (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (transformer): Sequential(\n",
       "        (0): Trans_depth(\n",
       "          (layers): ModuleList(\n",
       "            (0-3): 4 x TransformerBlock(\n",
       "              (mha): Res2Attention(\n",
       "                (res2): Res2Net(\n",
       "                  (inconv): Sequential(\n",
       "                    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (x3conv): Sequential(\n",
       "                    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "                    (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "                (hue): RGB2HSV()\n",
       "                (to_q): SepConv2d(\n",
       "                  (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (pointwise): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (to_k): SepConv2d(\n",
       "                  (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (pointwise): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (to_v): SepConv2d(\n",
       "                  (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "                  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (pointwise): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (linear): Linear(in_features=1, out_features=16, bias=False)\n",
       "                (ln_v): Linear(in_features=16, out_features=16, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (1): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (avg_pool): AdaptiveAvgPool2d(output_size=4)\n",
       "                (max_pool): AdaptiveMaxPool2d(output_size=4)\n",
       "                (out_attention): Linear(in_features=256, out_features=256, bias=False)\n",
       "                (drop): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (mlp_layers): Sequential(\n",
       "                  (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                  (2): Dropout(p=0.1, inplace=False)\n",
       "                  (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  (4): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Rearrange('b (h w) c -> b c h w', h=16, w=16)\n",
       "      )\n",
       "    )\n",
       "    (dconv_up3): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dconv_up2): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (dconv_up1): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (conv_last): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (upscore_bt): Upsample(scale_factor=16.0, mode='bilinear')\n",
       "    (upscore3): Upsample(scale_factor=8.0, mode='bilinear')\n",
       "    (upscore2): Upsample(scale_factor=4.0, mode='bilinear')\n",
       "    (fn_bt): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fn3): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fn2): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.utilities.cloud_io import load\n",
    "from models.flooding_model import WorldFloodsModel\n",
    "importlib.reload(flooding_model)\n",
    "\n",
    "model = WorldFloodsModel(config.model_params)\n",
    "# path_to_models = os.path.join(config.model_params.model_folder,config.experiment_name, \"model_rgbnir.pt\").replace(\"\\\\\",\"/\")\n",
    "path_to_models = os.path.join(config.model_params.model_folder,config.experiment_name, \"model_rgbnirh_worldflood2_res2vtunet_biou+dice.pt\").replace(\"\\\\\",\"/\")\n",
    "# model_rgbnir_epoch_4.pt\n",
    "\n",
    "print(path_to_models)\n",
    "model.load_state_dict(load(path_to_models))\n",
    "model\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cuda\") # comment this line if your machine does not have GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aggregate-retirement",
   "metadata": {
    "id": "aggregate-retirement",
    "outputId": "42a245d0-9ac3-443d-f297-d29e3e1b67c0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting model inference function\n",
      "Max tile size: 256\n"
     ]
    }
   ],
   "source": [
    "# from ml4floods.models.model_setup import get_model_inference_function\n",
    "from models.inference import get_model_inference_function\n",
    "\n",
    "inference_function = get_model_inference_function(model, config, apply_normalization=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "biological-howard",
   "metadata": {
    "id": "biological-howard"
   },
   "source": [
    "## Step 3: Helper functions for plotting and reading some demo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75edbdac-9d30-427d-9280-4100d9cfe507",
   "metadata": {
    "id": "robust-consequence",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio import plot as rasterioplt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from ml4floods.data.worldfloods.configs import BANDS_S2, CHANNELS_CONFIGURATIONS\n",
    "# from ml4floods.visualization.plot_utils import download_tiff\n",
    "from ml4floods.data.worldfloods import dataset\n",
    "import os\n",
    "    \n",
    "\n",
    "@torch.no_grad()\n",
    "def read_inference_pair(tiff_inputs:str, folder_ground_truth:str, \n",
    "                        window:Optional[Union[rasterio.windows.Window, Tuple[slice,slice]]], \n",
    "                        return_ground_truth: bool=False, channels:bool=None, \n",
    "                        folder_permanent_water:Optional[str]=None,\n",
    "                        cache_folder:Optional[str]=None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, rasterio.Affine]:\n",
    "    \"\"\"\n",
    "    Read a pair of layers from the worldfloods bucket and return them as Tensors to pass to a model, return the transform for plotting with lat/long\n",
    "    \n",
    "    Args:\n",
    "        tiff_inputs: filename for layer in worldfloods bucket\n",
    "        folder_ground_truth: folder name to be replaced by S2 in the input\n",
    "        window: window of layer to use\n",
    "        return_ground_truth: flag to indicate if paired gt layer should be returned\n",
    "        channels: list of channels to read from the image\n",
    "        folder_permanent_water: Folder with permanent water layer raster.\n",
    "        cache_folder: if provided and tiff_inputs are in a google bucket it will download the tiffs before opening them.\n",
    "    \n",
    "    Returns:\n",
    "        (torch_inputs, torch_targets, transform): inputs Tensor, gt Tensor, transform for plotting with lat/long\n",
    "    \"\"\"\n",
    "    \n",
    "    if cache_folder is not None and tiff_inputs.startswith(\"gs\"):\n",
    "        tiff_inputs = dataset.load_input(cache_folder, tiff_inputs, folder_ground_truth, folder_permanent_water)\n",
    "    \n",
    "    tiff_targets = tiff_inputs.replace(\"/S2/\", folder_ground_truth)\n",
    "\n",
    "    with rasterio.open(tiff_inputs, \"r\") as rst:\n",
    "        inputs = rst.read((np.array(channels) + 1).tolist(), window=window)\n",
    "        # Shifted transform based on the given window (used for plotting)\n",
    "        transform = rst.transform if window is None else rasterio.windows.transform(window, rst.transform)\n",
    "        torch_inputs = torch.Tensor(inputs.astype(np.float32)).unsqueeze(0)\n",
    "    \n",
    "    if folder_permanent_water is not None:\n",
    "        tiff_permanent_water = tiff_inputs.replace(\"/S2/\", folder_permanent_water)\n",
    "        with rasterio.open(tiff_permanent_water, \"r\") as rst:\n",
    "            permanent_water = rst.read(1, window=window)  \n",
    "            permanent_water = permanent_water.astype(float)\n",
    "            torch_permanent_water = torch.tensor(permanent_water)\n",
    "    else:\n",
    "        torch_permanent_water = torch.zeros_like(torch_inputs)\n",
    "        \n",
    "    if return_ground_truth:\n",
    "        with rasterio.open(tiff_targets, \"r\") as rst:\n",
    "            targets = rst.read(1, window=window)\n",
    "       \n",
    "        targets = targets.astype(float)\n",
    "        torch_targets = torch.tensor(targets).unsqueeze(0)\n",
    "    else:\n",
    "        torch_targets = torch.zeros_like(torch_inputs)\n",
    "    \n",
    "    return torch_inputs, torch_targets, torch_permanent_water, transform\n",
    "\n",
    "COLORS_WORLDFLOODS = np.array([[0, 0, 0], # invalid\n",
    "                               [139, 64, 0], # land\n",
    "                               [0, 0, 139], # water\n",
    "                               [220, 220, 220]], # cloud\n",
    "                              dtype=np.float32) / 255\n",
    "\n",
    "INTERPRETATION_WORLDFLOODS = [\"invalid\", \"land\", \"water\", \"cloud\"]\n",
    "\n",
    "COLORS_WORLDFLOODS_PERMANENT = np.array([[0, 0, 0], # 0: invalid\n",
    "                                         [139, 64, 0], # 1: land\n",
    "                                         [237, 0, 0], # 2: flood_water\n",
    "                                         [220, 220, 220], # 3: cloud\n",
    "                                         [0, 0, 139], # 4: permanent_water\n",
    "                                         [60, 85, 92]], # 5: seasonal_water\n",
    "                                        dtype=np.float32) / 255\n",
    "\n",
    "INTERPRETATION_WORLDFLOODS_PERMANENT = [\"invalid\", \"land\", \"flood water\", \"cloud\", \"permanent water\", \"seasonal water\"]\n",
    "\n",
    "def gt_with_permanent_water(gt: np.ndarray, permanent_water: np.ndarray)->np.ndarray:\n",
    "    \"\"\" Permanent water taken from: https://developers.google.com/earth-engine/datasets/catalog/JRC_GSW1_2_YearlyHistory\"\"\"\n",
    "    gt[(gt == 2) & (permanent_water == 3)] = 4 # set as permanent_water\n",
    "    gt[(gt == 2) & (permanent_water == 2)] = 5 # set as seasonal water\n",
    "        \n",
    "    return gt\n",
    "            \n",
    "\n",
    "def get_cmap_norm_colors(color_array, interpretation_array):\n",
    "    cmap_categorical = colors.ListedColormap(color_array)\n",
    "    norm_categorical = colors.Normalize(vmin=-.5,\n",
    "                                        vmax=color_array.shape[0]-.5)\n",
    "    patches = []\n",
    "    for c, interp in zip(color_array, interpretation_array):\n",
    "        patches.append(mpatches.Patch(color=c, label=interp))\n",
    "    \n",
    "    return cmap_categorical, norm_categorical, patches\n",
    "\n",
    "\n",
    "def plot_inference_set(inputs: torch.Tensor, targets: torch.Tensor, \n",
    "                       predictions: torch.Tensor, permanent_water: torch.Tensor, transform: rasterio.Affine, \n",
    "                       channel_configuration:str)->None:\n",
    "    \"\"\"\n",
    "    Plots inputs, targets and prediction into lat/long visualisation\n",
    "    \n",
    "    Args:\n",
    "        inputs: input Tensor\n",
    "        targets: gt target Tensor\n",
    "        prediction: predictions output by model (softmax, argmax already applied)\n",
    "        permanent_water: permanent water raster\n",
    "        transform: transform used to plot with lat/long\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(2,2,figsize=(16,16))\n",
    "    \n",
    "    inputs_show = inputs.cpu().numpy().squeeze()\n",
    "    targets_show = targets.cpu().numpy().squeeze()\n",
    "    permanent_water_show = permanent_water.numpy().squeeze()\n",
    "    \n",
    "    targets_show = gt_with_permanent_water(targets_show, permanent_water_show)\n",
    "    \n",
    "    \n",
    "    # Color categories {-1: invalid, 0: land, 1: water, 2: clouds}\n",
    "    \n",
    "    cmap_preds, norm_preds, patches_preds = get_cmap_norm_colors(COLORS_WORLDFLOODS, INTERPRETATION_WORLDFLOODS)\n",
    "    cmap_gt, norm_gt, patches_gt = get_cmap_norm_colors(COLORS_WORLDFLOODS_PERMANENT, INTERPRETATION_WORLDFLOODS_PERMANENT)\n",
    "    \n",
    "    # +1 because value 0 is invalid\n",
    "    prediction_show = (predictions + 1).cpu().numpy().astype(float)\n",
    "    \n",
    "    band_names_current_image = [BANDS_S2[iband] for iband in CHANNELS_CONFIGURATIONS[channel_configuration]]\n",
    "    \n",
    "    if all(b in band_names_current_image for b in [\"B4\", \"B3\", \"B2\"]):\n",
    "        bands = [band_names_current_image.index(b) for b in [\"B4\", \"B3\", \"B2\"]]\n",
    "        rgb = np.clip(inputs_show[bands, :, :]/3000.,0,1)\n",
    "        rasterioplt.show(rgb,transform=transform,ax=ax[0,0])\n",
    "        ax[0,0].set_title(\"RGB Composite\")\n",
    "    else:\n",
    "        print(\"Can't show RGB Composite image lacks bands\")\n",
    "    \n",
    "    if all(b in band_names_current_image for b in [\"B11\", \"B8\", \"B4\"]):\n",
    "        bands_false_composite = [BANDS_S2.index(b) for b in [\"B11\", \"B8\", \"B4\"]] # swir_1, nir, red composite\n",
    "        false_rgb = np.clip(inputs_show[bands_false_composite, :, :]/3000.,0,1)\n",
    "        rasterioplt.show(false_rgb,transform=transform,ax=ax[0,1])\n",
    "        ax[0,1].set_title(\"SWIR1,NIR,R Composite\")\n",
    "    else:\n",
    "        print(\"Can't show SWIR1,NIR,R Composite image lacks bands\")\n",
    "        \n",
    "    rasterioplt.show(targets_show,transform=transform,ax=ax[1,0], cmap=cmap_gt, norm=norm_gt,\n",
    "                     interpolation='nearest')\n",
    "    rasterioplt.show(prediction_show, transform=transform, ax=ax[1,1],cmap=cmap_preds, norm=norm_preds,\n",
    "                     interpolation='nearest')\n",
    "    \n",
    "    ax[1,0].set_title(\"Ground Truth\")\n",
    "    ax[1,0].legend(handles=patches_gt,\n",
    "                 loc='upper right')\n",
    "    \n",
    "    ax[1,1].set_title(\"Model prediction\")\n",
    "    ax[1,1].legend(handles=patches_preds,\n",
    "                   loc='upper right')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7f92896-67d5-4c26-88c1-7cfaf7bfc7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RasterioIOError",
     "evalue": "GS_SECRET_ACCESS_KEY+GS_ACCESS_KEY_ID, GS_OAUTH2_REFRESH_TOKEN or GOOGLE_APPLICATION_CREDENTIALS or GS_OAUTH2_PRIVATE_KEY+GS_OAUTH2_CLIENT_EMAIL and /home/hsu/.boto, or GS_NO_SIGN_REQUEST=YES configuration options not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_AWSInvalidCredentialsError\u001b[0m           Traceback (most recent call last)",
      "File \u001b[0;32mrasterio/_base.pyx:310\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_base.pyx:221\u001b[0m, in \u001b[0;36mrasterio._base.open_dataset\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_err.pyx:221\u001b[0m, in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_AWSInvalidCredentialsError\u001b[0m: GS_SECRET_ACCESS_KEY+GS_ACCESS_KEY_ID, GS_OAUTH2_REFRESH_TOKEN or GOOGLE_APPLICATION_CREDENTIALS or GS_OAUTH2_PRIVATE_KEY+GS_OAUTH2_CLIENT_EMAIL and /home/hsu/.boto, or GS_NO_SIGN_REQUEST=YES configuration options not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m channels \u001b[38;5;241m=\u001b[39m get_channel_configuration_bands(channel_configuration)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Read inputs\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m torch_inputs, transform \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiff_s2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m inference_function(torch_inputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# (num_classes, h, w)\u001b[39;00m\n",
      "File \u001b[0;32m~/pytorch/lib/python3.10/site-packages/ml4floods/data/worldfloods/dataset.py:266\u001b[0m, in \u001b[0;36mload_input\u001b[0;34m(tiff_input, channels, window)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_input\u001b[39m(tiff_input:\u001b[38;5;28mstr\u001b[39m, channels:Union[List[\u001b[38;5;28mint\u001b[39m],List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[1;32m    252\u001b[0m                window:Optional[rasterio\u001b[38;5;241m.\u001b[39mwindows\u001b[38;5;241m.\u001b[39mWindow]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, rasterio\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mAffine]:\n\u001b[1;32m    253\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    Reads from a tiff the specified channel and window.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mrasterio_open_read(tiff_input) \u001b[38;5;28;01mas\u001b[39;00m rst:\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(channels[\u001b[38;5;241m0\u001b[39m], numbers\u001b[38;5;241m.\u001b[39mNumber):\n\u001b[1;32m    268\u001b[0m             indexes \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39marray(channels) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/pytorch/lib/python3.10/site-packages/ml4floods/data/utils.py:148\u001b[0m, in \u001b[0;36mrasterio_open_read\u001b[0;34m(tifffile, requester_pays)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m REQUESTER_PAYS_AVAILABLE:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGS_USER_PROJECT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron, \\\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGS_USER_PROJECT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m env variable not found and requester_pays=True set a project name to read rasters from the bucket\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(i.e. -> export GS_USER_PROJECT=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmyprojectname\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mrasterio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtifffile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m src:\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m src\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/pytorch/lib/python3.10/site-packages/rasterio/env.py:451\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    448\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pytorch/lib/python3.10/site-packages/rasterio/__init__.py:304\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m path \u001b[38;5;241m=\u001b[39m _parse_path(raw_dataset_path)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 304\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    306\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m get_writer_for_path(path, driver\u001b[38;5;241m=\u001b[39mdriver)(\n\u001b[1;32m    307\u001b[0m         path, mode, driver\u001b[38;5;241m=\u001b[39mdriver, sharing\u001b[38;5;241m=\u001b[39msharing, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    308\u001b[0m     )\n",
      "File \u001b[0;32mrasterio/_base.pyx:312\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: GS_SECRET_ACCESS_KEY+GS_ACCESS_KEY_ID, GS_OAUTH2_REFRESH_TOKEN or GOOGLE_APPLICATION_CREDENTIALS or GS_OAUTH2_PRIVATE_KEY+GS_OAUTH2_CLIENT_EMAIL and /home/hsu/.boto, or GS_NO_SIGN_REQUEST=YES configuration options not defined"
     ]
    }
   ],
   "source": [
    "from ml4floods.models.model_setup import get_channel_configuration_bands\n",
    "from ml4floods.visualization import plot_utils\n",
    "from ml4floods.data.worldfloods import dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['GS_USER_PROJECT'] = 'myprojectname'\n",
    "channel_configuration = config.model_params.hyperparameters.channel_configuration\n",
    "\n",
    "dataset_folder = \"gs://ml4cc_data_lake/2_PROD/2_Mart/worldfloods_v1_0/\"\n",
    "event_id = \"RS2_20161008_Water_Extent_Corail_Pestel.tif\"\n",
    "tiff_s2 = os.path.join(dataset_folder, \"val\", \"S2\", event_id)\n",
    "tiff_gt = os.path.join(dataset_folder, \"val\", \"gt\", event_id)\n",
    "tiff_permanentwaterjrc = os.path.join(dataset_folder, \"val\", \"PERMANENTWATERJRC\", event_id)\n",
    "window = None\n",
    "channels = get_channel_configuration_bands(channel_configuration)\n",
    "\n",
    "# Read inputs\n",
    "torch_inputs, transform = dataset.load_input(tiff_s2, window=window, channels=channels)\n",
    "\n",
    "# Make predictions\n",
    "outputs = inference_function(torch_inputs.unsqueeze(0))[0] # (num_classes, h, w)\n",
    "prediction = torch.argmax(outputs, dim=0).long() # (h, w)\n",
    "\n",
    "# Mask invalid pixels\n",
    "mask_invalid = torch.all(torch_inputs == 0, dim=0)\n",
    "prediction+=1\n",
    "prediction[mask_invalid] = 0\n",
    "\n",
    "# Load GT and permanent water for plotting\n",
    "torch_targets, _ = dataset.load_input(tiff_gt, window=window, channels=[0])\n",
    "torch_permanent_water, _ = dataset.load_input(tiff_permanentwaterjrc, window=window, channels=[0])\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(2,2, figsize=(16,16))\n",
    "plot_utils.plot_rgb_image(torch_inputs, transform=transform, ax=axs[0,0])\n",
    "axs[0,0].set_title(\"RGB Composite\")\n",
    "plot_utils.plot_swirnirred_image(torch_inputs, transform=transform, ax=axs[0,1])\n",
    "axs[0,1].set_title(\"SWIR1,NIR,R Composite\")\n",
    "plot_utils.plot_gt_v1_with_permanent(torch_targets, torch_permanent_water, window=window, transform=transform, ax=axs[1,0])\n",
    "axs[1,0].set_title(\"Ground Truth with JRC Permanent\")\n",
    "plot_utils.plot_gt_v1(prediction.unsqueeze(0),transform=transform, ax=axs[1,1])\n",
    "axs[1,1].set_title(\"Model prediction\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "smart-porter",
   "metadata": {
    "id": "smart-porter"
   },
   "source": [
    "## Perform Inference using the `inference_function`\n",
    "\n",
    "The `inference_function` let us run the model on large tiles. For doing this it follows the tiling and stiching strategy\n",
    "described in https://arxiv.org/abs/1805.12219."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-hindu",
   "metadata": {
    "id": "southeast-hindu",
    "outputId": "5b0e016f-13c0-4ec7-b63b-bc124fc406a1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from ml4floods.models.model_setup import get_channel_configuration_bands\n",
    "from models.inference import get_channel_configuration_bands\n",
    "import os\n",
    "\n",
    "\n",
    "cache_folder = \"/mnt/d/Flooding/worldfloods_v1_0/tiffs_for_inference\"\n",
    "os.makedirs(cache_folder, exist_ok=True)\n",
    "\n",
    "channel_configuration = config.model_params.hyperparameters.channel_configuration\n",
    "# tiff_s2, window, channels = \"gs://ml4cc_data_lake/0_DEV/2_Mart/worldfloods_v1_0/val/S2/RS2_20161008_Water_Extent_Corail_Pestel.tif\", None, get_channel_configuration_bands(channel_configuration)\n",
    "tiff_s2, window, channels = \"gs://ml4cc_data_lake/2_PROD/2_Mart/worldfloods_v1_0/val/S2/RS2_20161008_Water_Extent_Corail_Pestel.tif\", None, get_channel_configuration_bands(channel_configuration)\n",
    "\n",
    "# Load the image and ground truth\n",
    "torch_inputs, torch_targets, \\\n",
    "   torch_permanent_water, transform = read_inference_pair(tiff_s2,folder_ground_truth=\"/gt/\", \n",
    "                                                          window=window, return_ground_truth=True, channels=channels,\n",
    "                                                          folder_permanent_water=\"/PERMANENTWATERJRC/\",\n",
    "                                                          cache_folder=cache_folder)\n",
    "\n",
    "# Compute the prediction\n",
    "outputs = inference_function(torch_inputs) # (batch_size, num_classes, h, w)\n",
    "prediction = torch.argmax(outputs, dim=1).long() # (batch_size, h, w)\n",
    "plot_inference_set(torch_inputs, torch_targets, prediction, torch_permanent_water, transform, \n",
    "                   channel_configuration=channel_configuration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acting-shipping",
   "metadata": {
    "id": "acting-shipping"
   },
   "source": [
    "### Bonus: vectorise the water masks and plot them\n",
    "\n",
    "In the code bellow `data_out` is a `GeoDataFrame` object. You can save it as a shapefile with [`save_file`](https://geopandas.org/docs/user_guide/io.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-transportation",
   "metadata": {
    "id": "disabled-transportation",
    "outputId": "c10bde8e-e00e-4d62-e8de-a3d6e0952e3c"
   },
   "outputs": [],
   "source": [
    "from ml4floods.models import postprocess\n",
    "from ml4floods.visualization import plot_utils\n",
    "import geopandas as gpd\n",
    "\n",
    "prob_water_mask = outputs[0,1].cpu().numpy()\n",
    "binary_water_mask = prob_water_mask>.5\n",
    "\n",
    "geoms_polygons = postprocess.get_water_polygons(binary_water_mask, transform=transform)\n",
    "\n",
    "data_out = gpd.GeoDataFrame({\"geometry\": geoms_polygons, \"id\": np.arange(len(geoms_polygons))})\n",
    "fig, ax = plt.subplots(1,1, figsize=(12, 12))\n",
    "data_out.plot(\"id\",legend=True,categorical=True,ax=ax,facecolor=\"None\",edgecolor=\"1\",linewidth=3)\n",
    "plot_utils.plot_s2_rbg_image(torch_inputs[0].cpu().numpy(), transform=transform, ax=ax, alpha=.6,\n",
    "                             channel_configuration=channel_configuration)\n",
    "data_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "roman-battery",
   "metadata": {
    "id": "roman-battery"
   },
   "source": [
    "## Lets try another image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-winner",
   "metadata": {
    "id": "correct-winner",
    "outputId": "cc43763c-8dc0-46b8-9558-81666cedafea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tiff_s2, window, channels = \"gs://ml4cc_data_lake/2_PROD/2_Mart/worldfloods_v1_0/test/S2/EMSR347_07ZOMBA_DEL_v2_observed_event_a.tif\", None, get_channel_configuration_bands(config.model_params.hyperparameters.channel_configuration)\n",
    "\n",
    "torch_inputs, torch_targets, torch_permanent_water, transform = read_inference_pair(tiff_s2, folder_ground_truth=\"/gt/\", \n",
    "                                                                                    window=window, \n",
    "                                                                                    return_ground_truth=True, channels=channels,\n",
    "                                                                                    folder_permanent_water=\"/PERMANENTWATERJRC/\",\n",
    "                                                                                    cache_folder=cache_folder)\n",
    "\n",
    "outputs = inference_function(torch_inputs) # (batch_size, num_classes, h, w)\n",
    "prediction = torch.argmax(outputs, dim=1).long() # (batch_size, h, w)\n",
    "plot_inference_set(torch_inputs, torch_targets, prediction, torch_permanent_water, transform,channel_configuration=channel_configuration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "neither-grass",
   "metadata": {
    "id": "neither-grass"
   },
   "source": [
    "### Bonus: vectorise the water masks and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-passion",
   "metadata": {
    "id": "collect-passion",
    "outputId": "76c7c969-8f6b-44a9-ad6b-eba06b1435c7"
   },
   "outputs": [],
   "source": [
    "prob_water_mask = outputs[0,1].cpu().numpy()\n",
    "binary_water_mask = prob_water_mask>.5\n",
    "\n",
    "geoms_polygons = postprocess.get_water_polygons(binary_water_mask, transform=transform)\n",
    "\n",
    "data_out = gpd.GeoDataFrame({\"geometry\": geoms_polygons, \"id\": np.arange(len(geoms_polygons))})\n",
    "fig, ax = plt.subplots(1,1, figsize=(12, 12))\n",
    "data_out.plot(\"id\",legend=True,categorical=True,ax=ax,facecolor=\"None\",edgecolor=\"1\",linewidth=3)\n",
    "plot_utils.plot_s2_rbg_image(torch_inputs[0].cpu().numpy(), transform=transform, ax=ax, alpha=.6,\n",
    "                             channel_configuration=channel_configuration)\n",
    "data_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fixed-fountain",
   "metadata": {
    "id": "fixed-fountain"
   },
   "source": [
    "## Lets try another image from the new data prepared by the Janitors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-attachment",
   "metadata": {
    "id": "roman-attachment",
    "outputId": "04680856-e30c-4f2e-d014-f5bfd386faa1"
   },
   "outputs": [],
   "source": [
    "import rasterio.windows \n",
    "window = rasterio.windows.Window(col_off=1543, row_off=247, \n",
    "                                 width=2000, height=2000)\n",
    "tiff_s2, channels = \"gs://ml4cc_data_lake/2_PROD/2_Mart/worldfloods_v1_0/tiffs_for_inference/GT_ADMIN_Dec-03-060534-2021_Conflict/V_1_1/EMSR501_AOI01_DEL_MONIT01_r1_v1.tif\", get_channel_configuration_bands(config.model_params.hyperparameters.channel_configuration)\n",
    "\n",
    "torch_inputs, torch_targets, torch_permanent_water, transform = read_inference_pair(tiff_s2, folder_ground_truth=\"/GT_ADMIN_Dec-03-232115-2021_Conflict/V_1_1/\", \n",
    "                                                                                    window=window, \n",
    "                                                                                    return_ground_truth=True, channels=channels,\n",
    "                                                                                    folder_permanent_water=\"/JRC/\",\n",
    "                                                                                    cache_folder=cache_folder)\n",
    "\n",
    "outputs = inference_function(torch_inputs) # (batch_size, num_classes, h, w)\n",
    "prediction = torch.argmax(outputs, dim=1).long() # (batch_size, h, w)\n",
    "plot_inference_set(torch_inputs, torch_targets, prediction, torch_permanent_water, transform,\n",
    "                   channel_configuration=channel_configuration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "incredible-commons",
   "metadata": {
    "id": "incredible-commons"
   },
   "source": [
    "### Bonus: vectorise the water masks and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-numbers",
   "metadata": {
    "id": "sporting-numbers",
    "outputId": "9ae40878-da35-4777-e56d-69656c7a5589"
   },
   "outputs": [],
   "source": [
    "prob_water_mask = outputs[0,1].cpu().numpy()\n",
    "binary_water_mask = prob_water_mask>.5\n",
    "\n",
    "geoms_polygons = postprocess.get_water_polygons(binary_water_mask, transform=transform)\n",
    "\n",
    "data_out = gpd.GeoDataFrame({\"geometry\": geoms_polygons, \"id\": np.arange(len(geoms_polygons))})\n",
    "fig, ax = plt.subplots(1,1, figsize=(12, 12))\n",
    "data_out.plot(\"id\",legend=False,categorical=True,ax=ax,facecolor=\"None\",edgecolor=\"1\",linewidth=3)\n",
    "plot_utils.plot_s2_rbg_image(torch_inputs[0].cpu().numpy(), transform=transform, ax=ax, alpha=.6,\n",
    "                            channel_configuration=channel_configuration)\n",
    "data_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c336e88-abd5-4547-bce9-7804d73ed443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "name": "HOWTO_Run_Inference_on_new_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
